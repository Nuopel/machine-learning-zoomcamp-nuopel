{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Homework [DRAFT]\n",
    "\n",
    "> Note: sometimes your answer doesn't match one of the options exactly.\n",
    "> That's fine.\n",
    "> Select the option that's closest to your solution.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "In this homework, we will use the lead scoring dataset Bank Marketing dataset. Download it from [here](https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv).\n",
    "\n",
    "Or you can do it with `wget`:\n",
    "\n",
    "```bash\n",
    "wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv\n",
    "```\n",
    "\n",
    "In this dataset our desired target for classification task will be `converted` variable - has the client signed up to the platform or not.\n",
    "\n",
    "### Data preparation\n",
    "\n",
    "* Check if the missing values are presented in the features.\n",
    "* If there are missing values:\n",
    "    * For caterogiral features, replace them with 'NA'\n",
    "    * For numerical features, replace with with 0.0\n"
   ],
   "id": "42e6bfa72daeb893"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T16:59:33.735469Z",
     "start_time": "2025-10-12T16:59:33.040217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score, mutual_info_score\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('course_lead_scoring.csv')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"APERÇU DES DONNÉES\")\n",
    "print(\"=\" * 80)\n",
    "print(df.head())\n",
    "print(\"\\nShape:\", df.shape)\n",
    "print(\"\\nTypes de données:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nValeurs manquantes:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Data preparation - Gestion des valeurs manquantes\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identifier les colonnes catégorielles et numériques\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Retirer 'converted' des colonnes catégorielles si présent\n",
    "if 'converted' in categorical_cols:\n",
    "    categorical_cols.remove('converted')\n",
    "if 'converted' in numerical_cols:\n",
    "    numerical_cols.remove('converted')\n",
    "\n",
    "print(f\"Colonnes catégorielles: {categorical_cols}\")\n",
    "print(f\"Colonnes numériques: {numerical_cols}\")\n",
    "\n",
    "# Remplacer les valeurs manquantes\n",
    "for col in categorical_cols:\n",
    "    df.fillna({col: 'NA'}, inplace=True)\n",
    "#\n",
    "for col in numerical_cols:\n",
    "    df.fillna({col:0.0}, inplace=True)\n",
    "\n",
    "print(\"\\nValeurs manquantes après traitement:\")\n",
    "print(df.isnull().sum())"
   ],
   "id": "dbad885ce5e1d71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "APERÇU DES DONNÉES\n",
      "================================================================================\n",
      "    lead_source    industry  number_of_courses_viewed  annual_income  \\\n",
      "0      paid_ads         NaN                         1        79450.0   \n",
      "1  social_media      retail                         1        46992.0   \n",
      "2        events  healthcare                         5        78796.0   \n",
      "3      paid_ads      retail                         2        83843.0   \n",
      "4      referral   education                         3        85012.0   \n",
      "\n",
      "  employment_status       location  interaction_count  lead_score  converted  \n",
      "0        unemployed  south_america                  4        0.94          1  \n",
      "1          employed  south_america                  1        0.80          0  \n",
      "2        unemployed      australia                  3        0.69          1  \n",
      "3               NaN      australia                  1        0.87          0  \n",
      "4     self_employed         europe                  3        0.62          1  \n",
      "\n",
      "Shape: (1462, 9)\n",
      "\n",
      "Types de données:\n",
      "lead_source                  object\n",
      "industry                     object\n",
      "number_of_courses_viewed      int64\n",
      "annual_income               float64\n",
      "employment_status            object\n",
      "location                     object\n",
      "interaction_count             int64\n",
      "lead_score                  float64\n",
      "converted                     int64\n",
      "dtype: object\n",
      "\n",
      "Valeurs manquantes:\n",
      "lead_source                 128\n",
      "industry                    134\n",
      "number_of_courses_viewed      0\n",
      "annual_income               181\n",
      "employment_status           100\n",
      "location                     63\n",
      "interaction_count             0\n",
      "lead_score                    0\n",
      "converted                     0\n",
      "dtype: int64\n",
      "\n",
      "================================================================================\n",
      "DATA PREPARATION\n",
      "================================================================================\n",
      "Colonnes catégorielles: ['lead_source', 'industry', 'employment_status', 'location']\n",
      "Colonnes numériques: ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n",
      "\n",
      "Valeurs manquantes après traitement:\n",
      "lead_source                 0\n",
      "industry                    0\n",
      "number_of_courses_viewed    0\n",
      "annual_income               0\n",
      "employment_status           0\n",
      "location                    0\n",
      "interaction_count           0\n",
      "lead_score                  0\n",
      "converted                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Question 1\n",
    "\n",
    "What is the most frequent observation (mode) for the column `industry`?\n",
    "\n",
    "- `NA`\n",
    "- `technology`\n",
    "- `healthcare`\n",
    "- `retail`\n"
   ],
   "id": "9cd134319ab0e040"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T16:59:33.789419Z",
     "start_time": "2025-10-12T16:59:33.781512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUESTION 1: Mode de la colonne 'industry'\")\n",
    "print(\"=\" * 80)\n",
    "industry_mode = df['industry'].mode()[0]\n",
    "print(f\"Mode de 'industry': {industry_mode}\")"
   ],
   "id": "709282de50a6421d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUESTION 1: Mode de la colonne 'industry'\n",
      "================================================================================\n",
      "Mode de 'industry': retail\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Question 2\n",
    "\n",
    "Create the [correlation matrix](https://www.google.com/search?q=correlation+matrix) for the numerical features of your dataset.\n",
    "In a correlation matrix, you compute the correlation coefficient between every pair of features.\n",
    "\n",
    "What are the two features that have the biggest correlation?\n",
    "\n",
    "- `interaction_count` and `lead_score`\n",
    "- `number_of_courses_viewed` and `lead_score`\n",
    "- `number_of_courses_viewed` and `interaction_count`\n",
    "- `annual_income` and `lead_score`\n",
    "\n",
    "Only consider the pairs above when answering this question.\n"
   ],
   "id": "49e967a2dfc03647"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T16:59:33.833768Z",
     "start_time": "2025-10-12T16:59:33.817824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Créer la matrice de corrélation pour les features numériques\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "print(\"\\nMatrice de corrélation:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Calculer les corrélations pour les paires spécifiées\n",
    "pairs = [\n",
    "    ('interaction_count', 'lead_score'),\n",
    "    ('number_of_courses_viewed', 'lead_score'),\n",
    "    ('number_of_courses_viewed', 'interaction_count'),\n",
    "    ('annual_income', 'lead_score')\n",
    "]\n",
    "\n",
    "print(\"\\nCorrélations pour les paires spécifiées:\")\n",
    "correlations = {}\n",
    "for feat1, feat2 in pairs:\n",
    "    if feat1 in correlation_matrix.columns and feat2 in correlation_matrix.columns:\n",
    "        corr = correlation_matrix.loc[feat1, feat2]\n",
    "        correlations[f\"{feat1} - {feat2}\"] = abs(corr)\n",
    "        print(f\"{feat1} et {feat2}: {corr:.4f} (abs: {abs(corr):.4f})\")\n",
    "\n",
    "max_corr_pair = max(correlations, key=correlations.get)\n",
    "print(f\"\\nPaire avec la plus grande corrélation: {max_corr_pair}\")\n",
    "\n",
    "\n"
   ],
   "id": "ccba90211a3949b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice de corrélation:\n",
      "                          number_of_courses_viewed  annual_income  \\\n",
      "number_of_courses_viewed                  1.000000       0.009770   \n",
      "annual_income                             0.009770       1.000000   \n",
      "interaction_count                        -0.023565       0.027036   \n",
      "lead_score                               -0.004879       0.015610   \n",
      "\n",
      "                          interaction_count  lead_score  \n",
      "number_of_courses_viewed          -0.023565   -0.004879  \n",
      "annual_income                      0.027036    0.015610  \n",
      "interaction_count                  1.000000    0.009888  \n",
      "lead_score                         0.009888    1.000000  \n",
      "\n",
      "Corrélations pour les paires spécifiées:\n",
      "interaction_count et lead_score: 0.0099 (abs: 0.0099)\n",
      "number_of_courses_viewed et lead_score: -0.0049 (abs: 0.0049)\n",
      "number_of_courses_viewed et interaction_count: -0.0236 (abs: 0.0236)\n",
      "annual_income et lead_score: 0.0156 (abs: 0.0156)\n",
      "\n",
      "Paire avec la plus grande corrélation: number_of_courses_viewed - interaction_count\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUESTION 2: Corrélation entre variables numériques\")\n",
    "print(\"=\" * 80)\n",
    "### Split the data\n",
    "\n",
    "* Split your data in train/val/test sets with 60%/20%/20% distribution.\n",
    "* Use Scikit-Learn for that (the `train_test_split` function) and set the seed to `42`.\n",
    "* Make sure that the target value `y` is not in your dataframe.\n",
    "\n",
    "### Question 3\n",
    "\n",
    "* Calculate the mutual information score between `y` and other categorical variables in the dataset. Use the training set only.\n",
    "* Round the scores to 2 decimals using `round(score, 2)`.\n",
    "\n",
    "Which of these variables has the biggest mutual information score?\n",
    "\n",
    "- `industry`\n",
    "- `location`\n",
    "- `lead_source`\n",
    "- `employment_status`\n"
   ],
   "id": "481971dce8b1c8a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T16:59:34.005696Z",
     "start_time": "2025-10-12T16:59:33.966337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split des données\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SPLIT DES DONNÉES (60/20/20)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Séparer y et X\n",
    "y = df['converted'].values\n",
    "df_features = df.drop('converted', axis=1)\n",
    "\n",
    "# Split train/temp (60/40)\n",
    "df_train, df_temp, y_train, y_temp = train_test_split(\n",
    "    df_features, y, test_size=0.4, random_state=42\n",
    ")\n",
    "\n",
    "# Split temp en val/test (50/50 du 40%, donc 20/20)\n",
    "df_val, df_test, y_val, y_test = train_test_split(\n",
    "    df_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(df_train)} samples\")\n",
    "print(f\"Validation set: {len(df_val)} samples\")\n",
    "print(f\"Test set: {len(df_test)} samples\")\n",
    "\n",
    "\n",
    "# QUESTION 3: Mutual Information Score\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUESTION 3: Mutual Information Score\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Recalculer les colonnes catégorielles sans 'converted'\n",
    "categorical_features = df_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "mi_scores = {}\n",
    "for col in categorical_features:\n",
    "    mi = mutual_info_score(y_train, df_train[col])\n",
    "    mi_scores[col] = round(mi, 2)\n",
    "    print(f\"{col}: {mi_scores[col]}\")\n",
    "\n",
    "max_mi_feature = max(mi_scores, key=mi_scores.get)\n",
    "print(f\"\\nFeature avec le plus grand MI score: {max_mi_feature} ({mi_scores[max_mi_feature]})\")\n"
   ],
   "id": "6a82149e5efde7dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SPLIT DES DONNÉES (60/20/20)\n",
      "================================================================================\n",
      "Train set: 877 samples\n",
      "Validation set: 292 samples\n",
      "Test set: 293 samples\n",
      "\n",
      "================================================================================\n",
      "QUESTION 3: Mutual Information Score\n",
      "================================================================================\n",
      "lead_source: 0.03\n",
      "industry: 0.02\n",
      "employment_status: 0.02\n",
      "location: 0.0\n",
      "\n",
      "Feature avec le plus grand MI score: lead_source (0.03)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Question 4\n",
    "\n",
    "* Now let's train a logistic regression.\n",
    "* Remember that we have several categorical variables in the dataset. Include them using one-hot encoding.\n",
    "* Fit the model on the training dataset.\n",
    "    - To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n",
    "    - `model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)`\n",
    "* Calculate the accuracy on the validation dataset and round it to 2 decimal digits.\n",
    "\n",
    "What accuracy did you get?\n",
    "\n",
    "- 0.64\n",
    "- 0.74\n",
    "- 0.84\n",
    "- 0.94\n"
   ],
   "id": "2a864f3a2e07a98a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T16:59:34.147250Z",
     "start_time": "2025-10-12T16:59:34.097638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# QUESTION 4: Logistic Regression avec one-hot encoding\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUESTION 4: Logistic Regression\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Convertir les dataframes en dictionnaires pour DictVectorizer\n",
    "train_dict = df_train.to_dict(orient='records')\n",
    "val_dict = df_val.to_dict(orient='records')\n",
    "test_dict = df_test.to_dict(orient='records')\n",
    "\n",
    "# One-hot encoding\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "X_val = dv.transform(val_dict)\n",
    "X_test = dv.transform(test_dict)\n",
    "\n",
    "print(f\"Shape après encoding - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Entraîner le modèle\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions et accuracy\n",
    "y_pred_val = model.predict(X_val)\n",
    "accuracy_q4 = accuracy_score(y_val, y_pred_val)\n",
    "accuracy_q4_rounded = round(accuracy_q4, 2)\n",
    "\n",
    "print(f\"Accuracy sur validation: {accuracy_q4:.6f}\")\n",
    "print(f\"Accuracy arrondie: {accuracy_q4_rounded}\")\n"
   ],
   "id": "3c1d5564cd30d763",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUESTION 4: Logistic Regression\n",
      "================================================================================\n",
      "Shape après encoding - Train: (877, 31), Val: (292, 31), Test: (293, 31)\n",
      "Accuracy sur validation: 0.743151\n",
      "Accuracy arrondie: 0.74\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### Question 5\n",
    "\n",
    "* Let's find the least useful feature using the *feature elimination* technique.\n",
    "* Train a model using the same features and parameters as in Q4 (without rounding).\n",
    "* Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n",
    "* For each feature, calculate the difference between the original accuracy and the accuracy without the feature.\n",
    "\n",
    "Which of following feature has the smallest difference?\n",
    "\n",
    "- `'industry'`\n",
    "- `'employment_status'`\n",
    "- `'lead_score'`\n",
    "\n",
    "> **Note**: The difference doesn't have to be positive.\n",
    "\n"
   ],
   "id": "6bc2f34fc90d6d35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T16:59:34.247580Z",
     "start_time": "2025-10-12T16:59:34.212164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# QUESTION 5: Feature Elimination\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUESTION 5: Feature Elimination\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "baseline_accuracy = accuracy_q4\n",
    "print(f\"Baseline accuracy: {baseline_accuracy:.6f}\")\n",
    "\n",
    "# Obtenir les noms des features après encoding\n",
    "feature_names = dv.get_feature_names_out()\n",
    "original_features = df_train.columns.tolist()\n",
    "\n",
    "differences = {}\n",
    "\n",
    "for feature in original_features:\n",
    "    # Trouver les colonnes correspondantes après one-hot encoding\n",
    "    feature_cols = [i for i, name in enumerate(feature_names) if name.startswith(feature + '=')]\n",
    "\n",
    "    if len(feature_cols) > 0:\n",
    "        # Créer X_train et X_val sans cette feature\n",
    "        X_train_reduced = np.delete(X_train, feature_cols, axis=1)\n",
    "        X_val_reduced = np.delete(X_val, feature_cols, axis=1)\n",
    "\n",
    "        # Entraîner le modèle sans cette feature\n",
    "        model_reduced = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "        model_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "        # Calculer l'accuracy\n",
    "        y_pred_reduced = model_reduced.predict(X_val_reduced)\n",
    "        accuracy_reduced = accuracy_score(y_val, y_pred_reduced)\n",
    "\n",
    "        # Calculer la différence\n",
    "        diff = abs(baseline_accuracy - accuracy_reduced)\n",
    "        differences[feature] = diff\n",
    "\n",
    "        print(f\"{feature}: accuracy={accuracy_reduced:.6f}, diff={diff:.6f}\")\n",
    "\n",
    "# Trouver la feature avec la plus petite différence\n",
    "min_diff_feature = min(differences, key=differences.get)\n",
    "print(f\"\\nFeature avec la plus petite différence: {min_diff_feature} ({differences[min_diff_feature]:.6f})\")\n"
   ],
   "id": "fea5320dfc798155",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUESTION 5: Feature Elimination\n",
      "================================================================================\n",
      "Baseline accuracy: 0.743151\n",
      "lead_source: accuracy=0.729452, diff=0.013699\n",
      "industry: accuracy=0.743151, diff=0.000000\n",
      "employment_status: accuracy=0.746575, diff=0.003425\n",
      "location: accuracy=0.743151, diff=0.000000\n",
      "\n",
      "Feature avec la plus petite différence: industry (0.000000)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Question 6\n",
    "\n",
    "* Now let's train a regularized logistic regression.\n",
    "* Let's try the following values of the parameter `C`: `[0.01, 0.1, 1, 10, 100]`.\n",
    "* Train models using all the features as in Q4.\n",
    "* Calculate the accuracy on the validation dataset and round it to 3 decimal digits.\n",
    "\n",
    "Which of these `C` leads to the best accuracy on the validation set?\n",
    "\n",
    "- 0.01\n",
    "- 0.1\n",
    "- 1\n",
    "- 10\n",
    "- 100\n"
   ],
   "id": "c9ac5b6f7d471aa0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T16:59:34.320261Z",
     "start_time": "2025-10-12T16:59:34.281022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# QUESTION 6: Regularized Logistic Regression\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUESTION 6: Regularized Logistic Regression\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "results = {}\n",
    "\n",
    "for C in C_values:\n",
    "    model_reg = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)\n",
    "    model_reg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_reg = model_reg.predict(X_val)\n",
    "    accuracy_reg = accuracy_score(y_val, y_pred_reg)\n",
    "    accuracy_reg_rounded = round(accuracy_reg, 3)\n",
    "\n",
    "    results[C] = accuracy_reg_rounded\n",
    "    print(f\"C={C}: accuracy={accuracy_reg_rounded}\")\n",
    "\n",
    "# Trouver le meilleur C\n",
    "best_accuracy = max(results.values())\n",
    "best_C = min([c for c, acc in results.items() if acc == best_accuracy])\n",
    "\n",
    "print(f\"\\nMeilleur C: {best_C} avec accuracy={best_accuracy}\")\n"
   ],
   "id": "331669c9d2d6de7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUESTION 6: Regularized Logistic Regression\n",
      "================================================================================\n",
      "C=0.01: accuracy=0.743\n",
      "C=0.1: accuracy=0.743\n",
      "C=1: accuracy=0.743\n",
      "C=10: accuracy=0.743\n",
      "C=100: accuracy=0.743\n",
      "\n",
      "Meilleur C: 0.01 avec accuracy=0.743\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "> **Note**: If there are multiple options, select the smallest `C`.\n",
    "\n",
    "## Submit the results\n",
    "\n",
    "* Submit your results here: https://courses.datatalks.club/ml-zoomcamp-2025/homework/hw03\n",
    "* If your answer doesn't match options exactly, select the closest one"
   ],
   "id": "7f7bf1a3694ad7db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# RÉSUMÉ DES RÉPONSES\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RÉSUMÉ DES RÉPONSES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Question 1 (Mode de 'industry'): {industry_mode}\")\n",
    "print(f\"Question 2 (Plus grande corrélation): {max_corr_pair}\")\n",
    "print(f\"Question 3 (Plus grand MI score): {max_mi_feature}\")\n",
    "print(f\"Question 4 (Accuracy): {accuracy_q4_rounded}\")\n",
    "print(f\"Question 5 (Feature avec plus petite différence): {min_diff_feature}\")\n",
    "print(f\"Question 6 (Meilleur C): {best_C}\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "7e5fbe8446378927"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
